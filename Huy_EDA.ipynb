{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"id":"Zv-VjXOVVxA8","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1748478333792,"user_tz":240,"elapsed":985,"user":{"displayName":"cao huy","userId":"01401569740397649543"}},"outputId":"0fa6babe-868b-43f7-b6c8-d26df7c4c85c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/data/test.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-2d346b487946>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the credit profile data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# df = pd.read_csv('/content/drive/MyDrive/Huy_ACE/Huy_Projects/test.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data loaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/test.csv'"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# Load the credit profile data\n","df = pd.read_csv('/content/data/test.csv')\n","# df = pd.read_csv('/content/drive/MyDrive/Huy_ACE/Huy_Projects/test.csv')\n","print(\"Data loaded successfully!\")\n","\n","# Drop the ID column if it exists\n","if 'ID' in df.columns:\n","    df = df.drop('ID', axis=1)\n","    print(\"ID column dropped.\")\n","else:\n","    print(\"ID column not found.\")\n","# Drop the ID column if it exists\n","if 'Unpaid_CO_84M ' in df.columns:\n","    df = df.drop('Unpaid_CO_84M ', axis=1)\n","    print(\"Unpaid_CO_84M  column dropped.\")\n","else:\n","    print(\"Unpaid_CO_84M  column not found.\")\n","# --- EDA Framework ---\n","\n","# Set options to display all columns and a large number of rows\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","\n","print(\"\\n--- EDA: Exploring Credit Risk Data ---\")\n","\n","# 1. Discuss the concept of consumer credit risk and define how it is calculated.\n","print(\"\\n1. Credit Risk Concept and Calculation:\")\n","print(\"Credit risk is the possibility of a borrower failing to repay a loan or meet their debt obligations. It's a crucial concern for lenders as it directly impacts their profitability.\")\n","print(\"Credit risk assessment often involves evaluating a borrower's creditworthiness based on factors like credit history, repayment capacity, income, loan terms, and collateral.\")\n","print(\"Quantifying credit risk can involve calculating metrics such as:\")\n","print(\"- Probability of Default (PD%): The likelihood that a borrower will default on their debt.\")\n","print(\"- Net Credit Loss ($NCL%): The total amount the lender is at risk for at the time of default.\")\n","\n","# 2. Explore the credit data\n","print(\"\\n2. Exploring the Credit Data:\")\n","print(\"\\nFirst, let's get a general overview of the data:\")\n","print(df.head())\n","print(\"\\nDataframe information:\")\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# Load the credit profile data\n","df = pd.read_csv('/content/data/test.csv')\n","# df = pd.read_csv('/content/drive/MyDrive/Huy_ACE/Huy_Projects/test.csv')\n","print(\"Data loaded successfully!\")\n","\n","# Drop the ID column if it exists\n","if 'ID' in df.columns:\n","    df = df.drop('ID', axis=1)\n","    print(\"ID column dropped.\")\n","else:\n","    print(\"ID column not found.\")\n","# Drop the ID column if it exists\n","if 'Unpaid_CO_84M ' in df.columns:\n","    df = df.drop('Unpaid_CO_84M ', axis=1)\n","    print(\"Unpaid_CO_84M  column dropped.\")\n","else:\n","    print(\"Unpaid_CO_84M  column not found.\")\n","# --- EDA Framework ---\n","\n","# Set options to display all columns and a large number of rows\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","\n","print(\"\\n--- EDA: Exploring Credit Risk Data ---\")\n","\n","# 1. Discuss the concept of consumer credit risk and define how it is calculated.\n","print(\"\\n1. Credit Risk Concept and Calculation:\")\n","print(\"Credit risk is the possibility of a borrower failing to repay a loan or meet their debt obligations. It's a crucial concern for lenders as it directly impacts their profitability.\")\n","print(\"Credit risk assessment often involves evaluating a borrower's creditworthiness based on factors like credit history, repayment capacity, income, loan terms, and collateral.\")\n","print(\"Quantifying credit risk can involve calculating metrics such as:\")\n","print(\"- Probability of Default (PD%): The likelihood that a borrower will default on their debt.\")\n","print(\"- Net Credit Loss ($NCL%): The total amount the lender is at risk for at the time of default.\")\n","\n","# 2. Explore the credit data\n","print(\"\\n2. Exploring the Credit Data:\")\n","print(\"\\nFirst, let's get a general overview of the data:\")\n","print(df.head())\n","print(\"\\nDataframe information:\")\n","df.info()\n","print(\"\\nSummary statistics of numerical features:\")\n","print(df.describe())\n","print(\"\\nValue counts of categorical features:\")\n","for column in df.select_dtypes(include='object').columns:\n","    print(f\"\\nValue counts for column '{column}':\")\n","    print(df[column].value_counts())\n","\n","# 3. Crosstab and pivot tables with two target variables\n","print(\"\\n3. Correlation Heatmaps with Target Variables Ever_ChargeOff and ChargeOff_Balance:\")\n","print(\"\\nTo simplify the analysis of correlations, we will generate two heatmaps showing the correlation of all numerical variables with each of the target variables.\")\n","\n","target_binary = 'Ever_ChargeOff'\n","target_continuous = 'ChargeOff_Balance'\n","\n","numerical_df = df.select_dtypes(include=np.number).copy()\n","\n","# Correlation with Ever_ChargeOff\n","correlation_with_ever_chargeoff = numerical_df.corr()[target_binary].sort_values(ascending=False)\n","\n","# Display the correlation table for Ever_ChargeOff\n","print(\"\\nCorrelation Table with Ever_ChargeOff:\")\n","print(correlation_with_ever_chargeoff.to_frame())\n","\n","# Correlation heatmap for Ever_ChargeOff\n","plt.figure(figsize=(8, 10))\n","sns.heatmap(correlation_with_ever_chargeoff.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","plt.title(f'Correlation with Ever_ChargeOff')\n","plt.show()\n","\n","# Correlation with ChargeOff_Balance\n","correlation_with_chargeoff_balance = numerical_df.corr()[target_continuous].sort_values(ascending=False)\n","\n","# Display the correlation table for ChargeOff_Balance\n","print(\"\\nCorrelation Table with ChargeOff_Balance:\")\n","print(correlation_with_chargeoff_balance.to_frame())\n","\n","# Correlation heatmap for ChargeOff_Balance\n","plt.figure(figsize=(8, 10))\n","sns.heatmap(correlation_with_chargeoff_balance.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","plt.title(f'Correlation with ChargeOff_Balance')\n","plt.show()\n","\n","# Drop the ChargeOff_Balance column if it exists due to abnormal high miss-match of Ever_ChargeOff and ChargeOff_Balance\n","if 'ChargeOff_Balance' in df.columns:\n","    df = df.drop('ChargeOff_Balance', axis=1)\n","    print(\"ChargeOff_Balance column dropped.\")\n","else:\n","    print(\"ChargeOff_Balance column not found.\")\n","\n","# 4. Outliers in credit data\n","print(\"\\n4. Outliers in Credit Data:\")\n","print(\"\\nOutliers are data points that significantly deviate from the rest of the data. In credit data, outliers can represent unusual applicant profiles or errors in data entry. They can skew statistical analyses and affect machine learning model performance.\")\n","\n","# 5. Finding outliers with cross tables\n","print(\"\\n5. Finding Outliers with Cross Tables:\")\n","print(\"\\nWhile cross tables primarily show relationships between categorical variables, unusual distributions within categories might hint at outliers in related numerical variables.\")\n","print(\"For example, if a specific category has a very high or low average loan amount compared to others, it could indicate potential outliers.\")\n","\n","print(\"\\nValue Frequency of Binary Columns:\")\n","for column in df.columns:\n","    if df[column].nunique() == 2:\n","        print(f\"\\nValue counts for binary column '{column}':\")\n","        print(df[column].value_counts())\n","print(\"\\nSome noticeable insight: 1.4% went bankruptcy before, 20% ever collection, meanwhile, 14%.1 ever charge-off, reasonable for NTC, young adult\")\n","\n","print(\"\\nExamining Distributions with Histograms and KDE Plots:\")\n","for column in numerical_df.columns:\n","  if numerical_df[column].nunique() > 2:\n","     plt.figure(figsize=(8, 6))\n","     sns.histplot(numerical_df[column], kde=True)\n","     plt.title(f'Distribution of {column}')\n","     plt.show()\n","  else:\n","      print(f\"\\nSkipping box plot for '{column}' as it is binary.\")\n","\n","\n","print(\"\\nFor column with null values like FICO, the histogram only distibution for populated values\")\n","\n","print(\"\\nHistogram is great to give overview for how values are distributed, but need 1.5Q Method to effetively identify outliner\")\n","print(\"\\n6.Identifying Outliers using IQR Method (Non-Binary Columns):\")\n","for column in numerical_df.columns:\n","    if numerical_df[column].nunique() > 2:\n","        Q1 = numerical_df[column].quantile(0.25)\n","        Q3 = numerical_df[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","        outliers_iqr = numerical_df[column][(numerical_df[column] < lower_bound) | (numerical_df[column] > upper_bound)]\n","        if not outliers_iqr.empty:\n","            print(f\"\\nPotential outliers in '{column}' (IQR method):\\n{outliers_iqr.head()}\") # head function only shows 5 examples of the outliners\n","        else:\n","            print(f\"\\nNo significant outliers found in '{column}' using IQR method.\")\n","    else:\n","        print(f\"\\nSkipping IQR outlier detection for binary column '{column}'.\")\n","\n","print(\"\\n7. Removing Outliers (IQR Method, Non-Binary Columns):\")\n","original_row_count = len(df)\n","rows_removed_per_column = {}\n","df_cleaned = df.copy()  # Create a copy of the DataFrame to avoid modifying the original during iteration\n","\n","# 'Ever_ChargeOff' is our target and 'Unpaid_Collection_Trades' has a limited range (0-3)\n","exclude_cols_outlier = ['Unpaid_Collection_Trades', 'Ever_ChargeOff', 'ChargeOff_Balance']\n","\n","# Iterate through columns in numerical_df that are NOT in the exclude list\n","# This is the corrected loop structure:\n","for column in numerical_df.columns:\n","    if column in exclude_cols_outlier:\n","        print(f\"\\nSkipping outlier removal for explicitly excluded column: '{column}'.\")\n","        continue # Skip to the next column in the loop\n","\n","    # Now apply your existing logic for outlier removal if not excluded and not binary\n","    if df_cleaned[column].nunique() > 2: # Use df_cleaned here as it's the one being modified\n","        Q1 = df_cleaned[column].quantile(0.25)\n","        Q3 = df_cleaned[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","\n","        outliers_mask = (df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound)\n","        outlier_indices = df_cleaned[outliers_mask].index\n","        rows_removed = len(outlier_indices)\n","\n","        if rows_removed > 0:\n","            df_cleaned = df_cleaned.drop(outlier_indices)\n","            rows_removed_per_column[column] = rows_removed\n","            print(f\"\\nRemoved {rows_removed} rows with IQR outliers in column '{column}'.\")\n","        else:\n","            print(f\"\\nNo IQR outliers found in column '{column}'.\")\n","    else:\n","        print(f\"\\nSkipping outlier removal for binary column '{column}'.\")\n","\n","\n","print(f\"\\n--- Summary of Rows Removed ---\")\n","total_rows_removed = original_row_count - len(df_cleaned)\n","print(f\"Original number of rows: {original_row_count}\")\n","print(f\"Number of rows after outlier removal: {len(df_cleaned)}\")\n","print(f\"Total number of rows removed: {total_rows_removed}\")\n","print(\"\\nRows removed per column (if any):\")\n","for column, count in rows_removed_per_column.items():\n","    print(f\"- '{column}': {count}\")\n","\n","print(\"\\nCleaned DataFrame (first 5 rows):\")\n","print(df_cleaned.head())\n","\n","# 7. Risk with missing data in loan data\n","print(\"\\n8. Risk with Missing Data in Loan Data:\")\n","print(\"\\nMissing data is a common problem in real-world datasets. In loan data, missing values can introduce bias and reduce the accuracy of analyses and models.\")\n","print(\"The risk associated with missing data depends on the extent and pattern of missingness. For example:\")\n","print(\"- Missing values in crucial features like income or credit score can significantly impact credit risk assessment.\")\n","print(\"- If missingness is systematic (related to other variables), it can introduce bias.\")\n","print(\"- High percentages of missing values in a column might render machine learning model prediction feature unreliable.\")\n","\n","# 8. Replacing missing credit data\n","print(\"\\n9. Replacing Missing Credit Data:\")\n","print(\"\\nSeveral techniques can be used to handle missing data:\")\n","print(\"- Imputation: Filling missing values with estimated values (e.g., mean, median, mode).\")\n","print(\"- More sophisticated imputation techniques (e.g., using machine learning models).\")\n","\n","print(\"\\nLet's check for missing values:\")\n","print(df.isnull().sum().sort_values(ascending=False))\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Assuming your DataFrame after cleaning is named 'df_cleaned'\n","\n","print(\"\\n10. Split data into Train, Validation, and Test Data:\")\n","\n","# Split the data into training, validation, and test sets\n","train_df, temp_df = train_test_split(df_cleaned, test_size=0.3, random_state=42) # 30% go to validation-test, 70% remain for training\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)     #break-down 50/50 of 30% for validation and test\n","\n","print(f\"Training data shape: {train_df.shape}\")\n","print(f\"Validation data shape: {val_df.shape}\")\n","print(f\"Test data shape: {test_df.shape}\")\n","\n","print(\"\\n--- Explanation of Data Splitting ---\")\n","print(\"\"\"\n","We split our data into three distinct datasets: training, validation, and test. This is a fundamental practice in machine learning to build robust and reliable models.\n","\n","**1. Training Data:**\n","   - This is the largest portion of our data and the one that the machine learning model directly learns from.\n","   - The model will adjust its internal parameters based on the patterns and relationships it finds in the training data.\n","   - Think of it as the 'textbook' the model studies to understand the underlying concepts.\n","\n","**2. Validation Data:**\n","   - This dataset is used to tune the model's hyperparameters and to get an unbiased estimate of the model's performance *during* the training process.\n","   - Hyperparameters are settings of the model that are not learned from the data but are set prior to training (e.g., the learning rate of an algorithm, the depth of a decision tree).\n","   - By evaluating the model on the validation set after each training epoch or after trying different hyperparameter settings, we can see how well the model generalizes to unseen data and avoid overfitting (where the model learns the training data too well and performs poorly on new data).\n","   - The validation set acts as a 'practice exam' that helps us make adjustments to the model before the final evaluation.\n","\n","**3. Test Data:**\n","   - This is a completely separate dataset that the model *never* sees during the training or hyperparameter tuning phases.\n","   - It serves as the final, unbiased evaluation of the model's performance on completely new, unseen data.\n","   - The test set simulates how well the model would perform in a real-world scenario.\n","   - We only evaluate the model on the test set *once*, after we have finalized our model through training and validation.\n","   - Think of the test set as the 'final exam' that gives us a true measure of the model's capabilities.\n","\n","By using this three-way split, we can build a model that not only learns from the data but also generalizes well to new data and provides a reliable estimate of its real-world performance.\n","\"\"\")\n","\n","# Impute missing FICO in the training data with a special value (e.g., 0)\n","special_fico_value = 999\n","train_df['FICO_Imputed'] = train_df['FICO'].fillna(special_fico_value).astype(int)\n","\n","# Verify the imputation in the training data\n","print(\"\\nMissing FICO values in training data before imputation:\", train_df['FICO'].isnull().sum())\n","print(\"Missing FICO values in training data after imputation:\", train_df['FICO_Imputed'].isnull().sum())\n","print(\"First few rows of training data with imputed FICO:\")\n","print(train_df[['FICO', 'FICO_Imputed', 'No_Hit']].head())\n","\n","# For demonstration, let's also do the same for validation and test sets *independently*\n","# In a real project, you would apply the *same* special value to these as well.\n","val_df['FICO_Imputed'] = val_df['FICO'].fillna(special_fico_value).astype(int)\n","test_df['FICO_Imputed'] = test_df['FICO'].fillna(special_fico_value).astype(int)\n","\n","print(\"\\nMissing FICO values in validation data after imputation:\", val_df['FICO_Imputed'].isnull().sum())\n","print(\"Missing FICO values in test data after imputation:\", test_df['FICO_Imputed'].isnull().sum())\n","\n","# now check Ever_ChargeOff rate in train data\n","print(\"\\nValue Frequency of Ever_ChargeOff in Train Data:\")\n","for column in train_df.columns:\n","    if column == 'Ever_ChargeOff':\n","        print(f\"\\nValue counts for Ever_ChargeOff '{column}':\")\n","        print(train_df[column].value_counts())\n","print(\"\\nSome noticeable insight:  17%.1 ever charge-off, reasonable for NTC, young adult\")\n","# remember to check whether undersampling for training test is needed\n","# At 4%, we're in the moderate imbalance zone. No need to undersample — as 20% is isually considered as stastically important — but we still need to address imbalance through model-aware techniques.\n","\n","###### END of EDA #######\n","\n","###### --- MODELING: Predicting Ever_ChargeOff --- ######\n","\n","print(\"\\n=== Step 11: Building ML Model to Predict Ever_ChargeOff ===\")\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.preprocessing import StandardScaler\n","import xgboost as xgb # Import xgboost\n","\n","# Drop target and non-informative columns\n","# Also drop FICO_V and No_Hit to avoid redundancy (FICO_Imputed encodes both)\n","drop_cols = ['Ever_ChargeOff', 'FICO', 'No_Hit']\n","feature_cols = [col for col in train_df.columns if col not in drop_cols]\n","\n","\n","# 2. Prepare X and y\n","X_train = train_df[feature_cols]\n","y_train = train_df[target_binary]\n","\n","X_val = val_df[feature_cols]\n","y_val = val_df[target_binary]\n","\n","X_test = test_df[feature_cols]\n","y_test = test_df[target_binary]\n","\n","# 3. Fit an XGBoost Classifier\n","xgb_model = xgb.XGBClassifier(\n","    n_estimators=100,\n","    max_depth=6,\n","    learning_rate=0.1,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    use_label_encoder=False,\n","    eval_metric='logloss',\n","    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n","    random_state=42 # Add random_state for reproducibility\n",")\n","\n","xgb_model.fit(X_train, y_train)\n","\n","# Predictions on validation\n","val_probs_xgb = xgb_model.predict_proba(X_val)[:, 1]\n","val_preds_xgb = (val_probs_xgb >= 0.5).astype(int)\n","\n","# Evaluation on validation\n","val_auc_xgb = roc_auc_score(y_val, val_probs_xgb)\n","val_report_xgb = classification_report(y_val, val_preds_xgb, output_dict=True)\n","val_conf_xgb = confusion_matrix(y_val, val_preds_xgb)\n","\n","# ROC Curve\n","fpr_xgb, tpr_xgb, _ = roc_curve(y_val, val_probs_xgb)\n","\n","# Test set evaluation\n","test_probs_xgb = xgb_model.predict_proba(X_test)[:, 1]\n","test_preds_xgb = (test_probs_xgb >= 0.5).astype(int)\n","test_auc_xgb = roc_auc_score(y_test, test_probs_xgb)\n","test_report_xgb = classification_report(y_test, test_preds_xgb, output_dict=True)\n","test_conf_xgb = confusion_matrix(y_test, test_preds_xgb)\n","\n","# Visualize ROC curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost AUC = {val_auc_xgb:.2f}')\n","plt.plot([0, 1], [0, 1], linestyle='--')\n","plt.title('ROC Curve - XGBoost (Validation)')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","import ace_tools as tools; tools.display_dataframe_to_user(name=\"XGBoost Evaluation - Validation\", dataframe=pd.DataFrame(val_report_xgb).T)\n","# 3. Fit a Random Forest Classifier\n","rf_model = RandomForestClassifier(\n","    n_estimators=100,    #  Train 100 decision trees in the ensemble. More trees improve stability and generalization. 100 is a common, reliable starting point for performance without being too slow.\n","    max_depth=6,         #  Each tree can go only 6 levels deep. Prevents overfitting by limiting tree complexity. Shallow trees generalize better. Helps model stay interpretable and faster. Depth 6 is a good balance for our credit data (likely enough to model interactions without memorizing).\n","    class_weight='balanced',  # Automatically adjusts weights to compensate for class imbalance. Without balancing, the model might ignore the minority class. This tells the algorithm to treat both classes as equally important by assigning higher weight to the rare class.\n","    random_state=42  # fixed seed for reproducibility\n",")\n","rf_model.fit(X_train, y_train)\n","\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","import matplotlib.pyplot as plt\n","\n","# Train a shallow decision tree for easy visualization\n","tree = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=42)\n","tree.fit(X_train, y_train)\n","\n","# Plot the decision tree\n","plt.figure(figsize=(14, 8))\n","plot_tree(tree, feature_names=X_train.columns, class_names=[\"No Charge-off\", \"Charge-off\"], filled=True)\n","plt.title(\"Sample Decision Tree (Max Depth = 3)\")\n","plt.show()\n","#from the plot, at the root, we see gini at 0.5 meaning the highest uncertainty, getting down the tree, we see gini decrease\n","\n","# 4. Predict the target variable \"Ever_ChargeOff\" on validation data\n","val_probs = rf_model.predict_proba(X_val)[:, 1]\n","val_pred_thresh = (val_probs >= 0.5).astype(int)\n","# for each application, model will print out the probability of default as a Percentage number:\n","# Use threshold 0.3 for classification. Anyone with a (default 50%) 30% or higher chance of charge-off is now flagged as a predicted charge-off (class 1)\n","\n","# 5. Evaluate model performance using thresholded predictions\n","print(\"\\n--- Validation Performance ---\")\n","print(confusion_matrix(y_val, val_pred_thresh))\n","print(classification_report(y_val, val_pred_thresh, digits=3))\n","\n","val_auc = roc_auc_score(y_val, val_probs)\n","print(f\"Validation AUC: {val_auc:.3f}\")\n","\n","# 6. ROC Curve (unchanged, uses probabilities)\n","fpr, tpr, thresholds = roc_curve(y_val, val_probs)\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f\"AUC = {val_auc:.2f}\")\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve on Validation Set\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","# 7. Evaluate on Test Set (can remain unchanged if no thresholding applied here)\n","test_preds = rf_model.predict(X_test)\n","test_probs = rf_model.predict_proba(X_test)[:, 1]\n","\n","print(\"\\n--- Test Set Performance ---\")\n","print(confusion_matrix(y_test, test_preds))\n","print(classification_report(y_test, test_preds, digits=3))\n","print(f\"Test AUC: {roc_auc_score(y_test, test_probs):.3f}\")\n","\n","print(\"\\nAs our intent of applying business acumen of prioritize detecting charge-off, our model with decent 60% Recall with test data, but in ex-change for a low precision and F-1, meaning we are auto-decline more people than trying to play risk with accepting them\")\n","print(\"\\nThis model is suitable for business that have high tolerance in giving our extra manual review and. Our model working as designed\")\n","\n","# 8. Feature importance (unchanged)\n","importances = rf_model.feature_importances_\n","feat_imp_df = pd.DataFrame({'Feature': feature_cols, 'Importance': importances})\n","feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(data=feat_imp_df.head(15), x='Importance', y='Feature')\n","plt.title(\"Top 15 Feature Importances (Random Forest)\")\n","plt.tight_layout()\n","plt.show()\n","print(\"\\nValue counts of categorical features:\")\n","for column in df_cleaned.select_dtypes(include='object').columns: # Check cleaned df\n","    print(f\"\\nValue counts for column '{column}':\")\n","    print(df_cleaned[column].value_counts())\n","\n","# XGBoost: nureau network, cautious: overfiting, based off current dataset: 26 variables\n","# compare Random Forest and XGBoost : thi thang nao co precision cao hon, AUC cao hon.\n","    print(df[column].value_counts())\n","\n","# 3. Crosstab and pivot tables with two target variables\n","print(\"\\n3. Correlation Heatmaps with Target Variables Ever_ChargeOff and ChargeOff_Balance:\")\n","print(\"\\nTo simplify the analysis of correlations, we will generate two heatmaps showing the correlation of all numerical variables with each of the target variables.\")\n","\n","target_binary = 'Ever_ChargeOff'\n","target_continuous = 'ChargeOff_Balance'\n","\n","numerical_df = df.select_dtypes(include=np.number).copy()\n","\n","# Correlation with Ever_ChargeOff\n","correlation_with_ever_chargeoff = numerical_df.corr()[target_binary].sort_values(ascending=False)\n","\n","# Display the correlation table for Ever_ChargeOff\n","print(\"\\nCorrelation Table with Ever_ChargeOff:\")\n","print(correlation_with_ever_chargeoff.to_frame())\n","\n","# Correlation heatmap for Ever_ChargeOff\n","plt.figure(figsize=(8, 10))\n","sns.heatmap(correlation_with_ever_chargeoff.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","plt.title(f'Correlation with Ever_ChargeOff')\n","plt.show()\n","\n","# Correlation with ChargeOff_Balance\n","correlation_with_chargeoff_balance = numerical_df.corr()[target_continuous].sort_values(ascending=False)\n","\n","# Display the correlation table for ChargeOff_Balance\n","print(\"\\nCorrelation Table with ChargeOff_Balance:\")\n","print(correlation_with_chargeoff_balance.to_frame())\n","\n","# Correlation heatmap for ChargeOff_Balance\n","plt.figure(figsize=(8, 10))\n","sns.heatmap(correlation_with_chargeoff_balance.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","plt.title(f'Correlation with ChargeOff_Balance')\n","plt.show()\n","\n","# Drop the ChargeOff_Balance column if it exists due to abnormal high miss-match of Ever_ChargeOff and ChargeOff_Balance\n","if 'ChargeOff_Balance' in df.columns:\n","    df = df.drop('ChargeOff_Balance', axis=1)\n","    print(\"ChargeOff_Balance column dropped.\")\n","else:\n","    print(\"ChargeOff_Balance column not found.\")\n","\n","# 4. Outliers in credit data\n","print(\"\\n4. Outliers in Credit Data:\")\n","print(\"\\nOutliers are data points that significantly deviate from the rest of the data. In credit data, outliers can represent unusual applicant profiles or errors in data entry. They can skew statistical analyses and affect machine learning model performance.\")\n","\n","# 5. Finding outliers with cross tables\n","print(\"\\n5. Finding Outliers with Cross Tables:\")\n","print(\"\\nWhile cross tables primarily show relationships between categorical variables, unusual distributions within categories might hint at outliers in related numerical variables.\")\n","print(\"For example, if a specific category has a very high or low average loan amount compared to others, it could indicate potential outliers.\")\n","\n","print(\"\\ Value Frequency of Binary Columns:\")\n","for column in df.columns:\n","    if df[column].nunique() == 2:\n","        print(f\"\\nValue counts for binary column '{column}':\")\n","        print(df[column].value_counts())\n","print(\"\\Some noticeable insight: 1.4% went bankruptcy before, 20% ever collection, meanwhile, 14%.1 ever charge-off, reasonable for NTC, young adult\")\n","\n","print(\"\\Examining Distributions with Histograms and KDE Plots:\")\n","for column in numerical_df.columns:\n","  if numerical_df[column].nunique() > 2:\n","     plt.figure(figsize=(8, 6))\n","     sns.histplot(numerical_df[column], kde=True)\n","     plt.title(f'Distribution of {column}')\n","     plt.show()\n","  else:\n","      print(f\"\\nSkipping box plot for '{column}' as it is binary.\")\n","\n","\n","print(\"\\For column with null values like FICO, the histogram only distibution for populated values\")\n","\n","print(\"\\Histogram is great to give overview for how values are distributed, but need 1.5Q Method to effetively identify outliner\")\n","print(\"\\n6.Identifying Outliers using IQR Method (Non-Binary Columns):\")\n","for column in numerical_df.columns:\n","    if numerical_df[column].nunique() > 2:\n","        Q1 = numerical_df[column].quantile(0.25)\n","        Q3 = numerical_df[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","        outliers_iqr = numerical_df[column][(numerical_df[column] < lower_bound) | (numerical_df[column] > upper_bound)]\n","        if not outliers_iqr.empty:\n","            print(f\"\\nPotential outliers in '{column}' (IQR method):\\n{outliers_iqr.head()}\") # head function only shows 5 examples of the outliners\n","        else:\n","            print(f\"\\nNo significant outliers found in '{column}' using IQR method.\")\n","    else:\n","        print(f\"\\nSkipping IQR outlier detection for binary column '{column}'.\")\n","\n","print(\"\\n7. Removing Outliers (IQR Method, Non-Binary Columns):\")\n","original_row_count = len(df)\n","rows_removed_per_column = {}\n","df_cleaned = df.copy()  # Create a copy of the DataFrame to avoid modifying the original during iteration\n","\n","# 'Ever_ChargeOff' is our target and 'Unpaid_Collection_Trades' has a limited range (0-3)\n","exclude_cols_outlier = ['Unpaid_Collection_Trades', 'Ever_ChargeOff', 'ChargeOff_Balance']\n","\n","# Iterate through columns in numerical_df that are NOT in the exclude list\n","# This is the corrected loop structure:\n","for column in numerical_df.columns:\n","    if column in exclude_cols_outlier:\n","        print(f\"\\nSkipping outlier removal for explicitly excluded column: '{column}'.\")\n","        continue # Skip to the next column in the loop\n","\n","    # Now apply your existing logic for outlier removal if not excluded and not binary\n","    if df_cleaned[column].nunique() > 2: # Use df_cleaned here as it's the one being modified\n","        Q1 = df_cleaned[column].quantile(0.25)\n","        Q3 = df_cleaned[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","\n","        outliers_mask = (df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound)\n","        outlier_indices = df_cleaned[outliers_mask].index\n","        rows_removed = len(outlier_indices)\n","\n","        if rows_removed > 0:\n","            df_cleaned = df_cleaned.drop(outlier_indices)\n","            rows_removed_per_column[column] = rows_removed\n","            print(f\"\\nRemoved {rows_removed} rows with IQR outliers in column '{column}'.\")\n","        else:\n","            print(f\"\\nNo IQR outliers found in column '{column}'.\")\n","    else:\n","        print(f\"\\nSkipping outlier removal for binary column '{column}'.\")\n","\n","\n","print(f\"\\n--- Summary of Rows Removed ---\")\n","total_rows_removed = original_row_count - len(df_cleaned)\n","print(f\"Original number of rows: {original_row_count}\")\n","print(f\"Number of rows after outlier removal: {len(df_cleaned)}\")\n","print(f\"Total number of rows removed: {total_rows_removed}\")\n","print(\"\\nRows removed per column (if any):\")\n","for column, count in rows_removed_per_column.items():\n","    print(f\"- '{column}': {count}\")\n","\n","print(\"\\nCleaned DataFrame (first 5 rows):\")\n","print(df_cleaned.head())\n","\n","# 7. Risk with missing data in loan data\n","print(\"\\n8. Risk with Missing Data in Loan Data:\")\n","print(\"\\nMissing data is a common problem in real-world datasets. In loan data, missing values can introduce bias and reduce the accuracy of analyses and models.\")\n","print(\"The risk associated with missing data depends on the extent and pattern of missingness. For example:\")\n","print(\"- Missing values in crucial features like income or credit score can significantly impact credit risk assessment.\")\n","print(\"- If missingness is systematic (related to other variables), it can introduce bias.\")\n","print(\"- High percentages of missing values in a column might render machine learning model prediction feature unreliable.\")\n","\n","# 8. Replacing missing credit data\n","print(\"\\n9. Replacing Missing Credit Data:\")\n","print(\"\\nSeveral techniques can be used to handle missing data:\")\n","print(\"- Imputation: Filling missing values with estimated values (e.g., mean, median, mode).\")\n","print(\"- More sophisticated imputation techniques (e.g., using machine learning models).\")\n","\n","print(\"\\nLet's check for missing values:\")\n","print(df.isnull().sum().sort_values(ascending=False))\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Assuming your DataFrame after cleaning is named 'df_cleaned'\n","\n","print(\"\\n10. Split data into Train, Validation, and Test Data:\")\n","\n","# Split the data into training, validation, and test sets\n","train_df, temp_df = train_test_split(df_cleaned, test_size=0.3, random_state=42) # 30% go to validation-test, 70% remain for training\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)     #break-down 50/50 of 30% for validation and test\n","\n","print(f\"Training data shape: {train_df.shape}\")\n","print(f\"Validation data shape: {val_df.shape}\")\n","print(f\"Test data shape: {test_df.shape}\")\n","\n","print(\"\\n--- Explanation of Data Splitting ---\")\n","print(\"\"\"\n","We split our data into three distinct datasets: training, validation, and test. This is a fundamental practice in machine learning to build robust and reliable models.\n","\n","**1. Training Data:**\n","   - This is the largest portion of our data and the one that the machine learning model directly learns from.\n","   - The model will adjust its internal parameters based on the patterns and relationships it finds in the training data.\n","   - Think of it as the 'textbook' the model studies to understand the underlying concepts.\n","\n","**2. Validation Data:**\n","   - This dataset is used to tune the model's hyperparameters and to get an unbiased estimate of the model's performance *during* the training process.\n","   - Hyperparameters are settings of the model that are not learned from the data but are set prior to training (e.g., the learning rate of an algorithm, the depth of a decision tree).\n","   - By evaluating the model on the validation set after each training epoch or after trying different hyperparameter settings, we can see how well the model generalizes to unseen data and avoid overfitting (where the model learns the training data too well and performs poorly on new data).\n","   - The validation set acts as a 'practice exam' that helps us make adjustments to the model before the final evaluation.\n","\n","**3. Test Data:**\n","   - This is a completely separate dataset that the model *never* sees during the training or hyperparameter tuning phases.\n","   - It serves as the final, unbiased evaluation of the model's performance on completely new, unseen data.\n","   - The test set simulates how well the model would perform in a real-world scenario.\n","   - We only evaluate the model on the test set *once*, after we have finalized our model through training and validation.\n","   - Think of the test set as the 'final exam' that gives us a true measure of the model's capabilities.\n","\n","By using this three-way split, we can build a model that not only learns from the data but also generalizes well to new data and provides a reliable estimate of its real-world performance.\n","\"\"\")\n","\n","# Impute missing FICO in the training data with a special value (e.g., 0)\n","special_fico_value = 999\n","train_df['FICO_Imputed'] = train_df['FICO'].fillna(special_fico_value).astype(int)\n","\n","# Verify the imputation in the training data\n","print(\"\\nMissing FICO values in training data before imputation:\", train_df['FICO'].isnull().sum())\n","print(\"Missing FICO values in training data after imputation:\", train_df['FICO_Imputed'].isnull().sum())\n","print(\"First few rows of training data with imputed FICO:\")\n","print(train_df[['FICO', 'FICO_Imputed', 'No_Hit']].head())\n","\n","# For demonstration, let's also do the same for validation and test sets *independently*\n","# In a real project, you would apply the *same* special value to these as well.\n","val_df['FICO_Imputed'] = val_df['FICO'].fillna(special_fico_value).astype(int)\n","test_df['FICO_Imputed'] = test_df['FICO'].fillna(special_fico_value).astype(int)\n","\n","print(\"\\nMissing FICO values in validation data after imputation:\", val_df['FICO_Imputed'].isnull().sum())\n","print(\"Missing FICO values in test data after imputation:\", test_df['FICO_Imputed'].isnull().sum())\n","\n","# now check Ever_ChargeOff rate in train data\n","print(\"\\ Value Frequency of Ever_ChargeOff in Train Data:\")\n","for column in train_df.columns:\n","    if column == 'Ever_ChargeOff':\n","        print(f\"\\nValue counts for Ever_ChargeOff '{column}':\")\n","        print(train_df[column].value_counts())\n","print(\"\\Some noticeable insight:  17%.1 ever charge-off, reasonable for NTC, young adult\")\n","# remember to check whether undersampling for training test is needed\n","# At 4%, we're in the moderate imbalance zone. No need to undersample — as 20% is isually considered as stastically important — but we still need to address imbalance through model-aware techniques.\n","\n","###### END of EDA #######\n","\n","###### --- MODELING: Predicting Ever_ChargeOff --- ######\n","\n","print(\"\\n=== Step 11: Building ML Model to Predict Ever_ChargeOff ===\")\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.preprocessing import StandardScaler\n","\n","# Drop target and non-informative columns\n","# Also drop FICO_V and No_Hit to avoid redundancy (FICO_Imputed encodes both)\n","drop_cols = ['Ever_ChargeOff', 'FICO', 'No_Hit']\n","feature_cols = [col for col in train_df.columns if col not in drop_cols]\n","\n","\n","# 2. Prepare X and y\n","X_train = train_df[feature_cols]\n","y_train = train_df[target_binary]\n","\n","X_val = val_df[feature_cols]\n","y_val = val_df[target_binary]\n","\n","X_test = test_df[feature_cols]\n","y_test = test_df[target_binary]\n","xgb_model = xgb.XGBClassifier(\n","    n_estimators=100,\n","    max_depth=6,\n","    learning_rate=0.1,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    use_label_encoder=False,\n","    eval_metric='logloss',\n","    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum()\n",")\n","\n","xgb_model.fit(X_train, y_train)\n","\n","# Predictions on validation\n","val_probs_xgb = xgb_model.predict_proba(X_val)[:, 1]\n","val_preds_xgb = (val_probs_xgb >= 0.5).astype(int)\n","\n","# Evaluation on validation\n","val_auc_xgb = roc_auc_score(y_val, val_probs_xgb)\n","val_report_xgb = classification_report(y_val, val_preds_xgb, output_dict=True)\n","val_conf_xgb = confusion_matrix(y_val, val_preds_xgb)\n","\n","# ROC Curve\n","fpr_xgb, tpr_xgb, _ = roc_curve(y_val, val_probs_xgb)\n","\n","# Test set evaluation\n","test_probs_xgb = xgb_model.predict_proba(X_test)[:, 1]\n","test_preds_xgb = (test_probs_xgb >= 0.5).astype(int)\n","test_auc_xgb = roc_auc_score(y_test, test_probs_xgb)\n","test_report_xgb = classification_report(y_test, test_preds_xgb, output_dict=True)\n","test_conf_xgb = confusion_matrix(y_test, test_preds_xgb)\n","\n","# Visualize ROC curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost AUC = {val_auc_xgb:.2f}')\n","plt.plot([0, 1], [0, 1], linestyle='--')\n","plt.title('ROC Curve - XGBoost (Validation)')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","import ace_tools as tools; tools.display_dataframe_to_user(name=\"XGBoost Evaluation - Validation\", dataframe=pd.DataFrame(val_report_xgb).T)\n","# 3. Fit a Random Forest Classifier\n","rf_model = RandomForestClassifier(\n","    n_estimators=100,    #  Train 100 decision trees in the ensemble. More trees improve stability and generalization. 100 is a common, reliable starting point for performance without being too slow.\n","    max_depth=6,         #  Each tree can go only 6 levels deep. Prevents overfitting by limiting tree complexity. Shallow trees generalize better. Helps model stay interpretable and faster. Depth 6 is a good balance for our credit data (likely enough to model interactions without memorizing).\n","    class_weight='balanced',  # Automatically adjusts weights to compensate for class imbalance. Without balancing, the model might ignore the minority class. This tells the algorithm to treat both classes as equally important by assigning higher weight to the rare class.\n","    random_state=42  # fixed seed for reproducibility\n",")\n","rf_model.fit(X_train, y_train)\n","\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","import matplotlib.pyplot as plt\n","\n","# Train a shallow decision tree for easy visualization\n","tree = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=42)\n","tree.fit(X_train, y_train)\n","\n","# Plot the decision tree\n","plt.figure(figsize=(14, 8))\n","plot_tree(tree, feature_names=X_train.columns, class_names=[\"No Charge-off\", \"Charge-off\"], filled=True)\n","plt.title(\"Sample Decision Tree (Max Depth = 3)\")\n","plt.show()\n","#from the plot, at the root, we see gini at 0.5 meaning the highest uncertainty, getting down the tree, we see gini decrease\n","\n","# 4. Predict the target variable \"Ever_ChargeOff\" on validation data\n","val_probs = rf_model.predict_proba(X_val)[:, 1]\n","val_pred_thresh = (val_probs >= 0.5).astype(int)\n","# for each application, model will print out the probability of default as a Percentage number:\n","# Use threshold 0.3 for classification. Anyone with a (default 50%) 30% or higher chance of charge-off is now flagged as a predicted charge-off (class 1)\n","\n","# 5. Evaluate model performance using thresholded predictions\n","print(\"\\n--- Validation Performance ---\")\n","print(confusion_matrix(y_val, val_pred_thresh))\n","print(classification_report(y_val, val_pred_thresh, digits=3))\n","\n","val_auc = roc_auc_score(y_val, val_probs)\n","print(f\"Validation AUC: {val_auc:.3f}\")\n","\n","# 6. ROC Curve (unchanged, uses probabilities)\n","fpr, tpr, thresholds = roc_curve(y_val, val_probs)\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f\"AUC = {val_auc:.2f}\")\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve on Validation Set\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","# 7. Evaluate on Test Set (can remain unchanged if no thresholding applied here)\n","test_preds = rf_model.predict(X_test)\n","test_probs = rf_model.predict_proba(X_test)[:, 1]\n","\n","print(\"\\n--- Test Set Performance ---\")\n","print(confusion_matrix(y_test, test_preds))\n","print(classification_report(y_test, test_preds, digits=3))\n","print(f\"Test AUC: {roc_auc_score(y_test, test_probs):.3f}\")\n","\n","print(\"\\As our intent of applying business acumen of prioritize detecting charge-off, our model with decent 60% Recall with test data, but in ex-change for a low precision and F-1, meaning we are auto-decline more people than trying to play risk with accepting them\")\n","print(\"\\This model is suitable for business that have high tolerance in giving our extra manual review and. Our model working as designed\")\n","\n","# 8. Feature importance (unchanged)\n","importances = rf_model.feature_importances_\n","feat_imp_df = pd.DataFrame({'Feature': feature_cols, 'Importance': importances})\n","feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(data=feat_imp_df.head(15), x='Importance', y='Feature')\n","plt.title(\"Top 15 Feature Importances (Random Forest)\")\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# Load the credit profile data\n","df = pd.read_csv('/content/data/test.csv')\n","# df = pd.read_csv('/content/drive/MyDrive/Huy_ACE/Huy_Projects/test.csv')\n","print(\"Data loaded successfully!\")\n","\n","# Drop the ID column if it exists\n","if 'ID' in df.columns:\n","    df = df.drop('ID', axis=1)\n","    print(\"ID column dropped.\")\n","else:\n","    print(\"ID column not found.\")\n","# Drop the ID column if it exists\n","if 'Unpaid_CO_84M ' in df.columns:\n","    df = df.drop('Unpaid_CO_84M ', axis=1)\n","    print(\"Unpaid_CO_84M  column dropped.\")\n","else:\n","    print(\"Unpaid_CO_84M  column not found.\")\n","# --- EDA Framework ---\n","\n","# Set options to display all columns and a large number of rows\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","\n","print(\"\\n--- EDA: Exploring Credit Risk Data ---\")\n","\n","# 1. Discuss the concept of consumer credit risk and define how it is calculated.\n","print(\"\\n1. Credit Risk Concept and Calculation:\")\n","print(\"Credit risk is the possibility of a borrower failing to repay a loan or meet their debt obligations. It's a crucial concern for lenders as it directly impacts their profitability.\")\n","print(\"Credit risk assessment often involves evaluating a borrower's creditworthiness based on factors like credit history, repayment capacity, income, loan terms, and collateral.\")\n","print(\"Quantifying credit risk can involve calculating metrics such as:\")\n","print(\"- Probability of Default (PD%): The likelihood that a borrower will default on their debt.\")\n","print(\"- Net Credit Loss ($NCL%): The total amount the lender is at risk for at the time of default.\")\n","\n","# 2. Explore the credit data\n","print(\"\\n2. Exploring the Credit Data:\")\n","print(\"\\nFirst, let's get a general overview of the data:\")\n","print(df.head())\n","print(\"\\nDataframe information:\")\n","df.info()\n","print(\"\\nSummary statistics of numerical features:\")\n","print(df.describe())\n","print(\"\\nValue counts of categorical features:\")\n","for column in df.select_dtypes(include='object').columns:\n","    print(f\"\\nValue counts for column '{column}':\")\n","    print(df[column].value_counts())\n","\n","# 3. Crosstab and pivot tables with two target variables\n","print(\"\\n3. Correlation Heatmaps with Target Variables Ever_ChargeOff and ChargeOff_Balance:\")\n","print(\"\\nTo simplify the analysis of correlations, we will generate two heatmaps showing the correlation of all numerical variables with each of the target variables.\")\n","\n","target_binary = 'Ever_ChargeOff'\n","target_continuous = 'ChargeOff_Balance'\n","\n","numerical_df = df.select_dtypes(include=np.number).copy()\n","\n","# Correlation with Ever_ChargeOff\n","correlation_with_ever_chargeoff = numerical_df.corr()[target_binary].sort_values(ascending=False)\n","\n","# Display the correlation table for Ever_ChargeOff\n","print(\"\\nCorrelation Table with Ever_ChargeOff:\")\n","print(correlation_with_ever_chargeoff.to_frame())\n","\n","# Correlation heatmap for Ever_ChargeOff\n","plt.figure(figsize=(8, 10))\n","sns.heatmap(correlation_with_ever_chargeoff.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","plt.title(f'Correlation with Ever_ChargeOff')\n","plt.show()\n","\n","# Correlation with ChargeOff_Balance\n","correlation_with_chargeoff_balance = numerical_df.corr()[target_continuous].sort_values(ascending=False)\n","\n","# Display the correlation table for ChargeOff_Balance\n","print(\"\\nCorrelation Table with ChargeOff_Balance:\")\n","print(correlation_with_chargeoff_balance.to_frame())\n","\n","# Correlation heatmap for ChargeOff_Balance\n","plt.figure(figsize=(8, 10))\n","sns.heatmap(correlation_with_chargeoff_balance.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","plt.title(f'Correlation with ChargeOff_Balance')\n","plt.show()\n","\n","# Drop the ChargeOff_Balance column if it exists due to abnormal high miss-match of Ever_ChargeOff and ChargeOff_Balance\n","if 'ChargeOff_Balance' in df.columns:\n","    df = df.drop('ChargeOff_Balance', axis=1)\n","    print(\"ChargeOff_Balance column dropped.\")\n","else:\n","    print(\"ChargeOff_Balance column not found.\")\n","\n","# 4. Outliers in credit data\n","print(\"\\n4. Outliers in Credit Data:\")\n","print(\"\\nOutliers are data points that significantly deviate from the rest of the data. In credit data, outliers can represent unusual applicant profiles or errors in data entry. They can skew statistical analyses and affect machine learning model performance.\")\n","\n","# 5. Finding outliers with cross tables\n","print(\"\\n5. Finding Outliers with Cross Tables:\")\n","print(\"\\nWhile cross tables primarily show relationships between categorical variables, unusual distributions within categories might hint at outliers in related numerical variables.\")\n","print(\"For example, if a specific category has a very high or low average loan amount compared to others, it could indicate potential outliers.\")\n","\n","print(\"\\nValue Frequency of Binary Columns:\")\n","for column in df.columns:\n","    if df[column].nunique() == 2:\n","        print(f\"\\nValue counts for binary column '{column}':\")\n","        print(df[column].value_counts())\n","print(\"\\nSome noticeable insight: 1.4% went bankruptcy before, 20% ever collection, meanwhile, 14%.1 ever charge-off, reasonable for NTC, young adult\")\n","\n","print(\"\\nExamining Distributions with Histograms and KDE Plots:\")\n","for column in numerical_df.columns:\n","  if numerical_df[column].nunique() > 2:\n","     plt.figure(figsize=(8, 6))\n","     sns.histplot(numerical_df[column], kde=True)\n","     plt.title(f'Distribution of {column}')\n","     plt.show()\n","  else:\n","      print(f\"\\nSkipping box plot for '{column}' as it is binary.\")\n","\n","\n","print(\"\\nFor column with null values like FICO, the histogram only distibution for populated values\")\n","\n","print(\"\\nHistogram is great to give overview for how values are distributed, but need 1.5Q Method to effetively identify outliner\")\n","print(\"\\n6.Identifying Outliers using IQR Method (Non-Binary Columns):\")\n","for column in numerical_df.columns:\n","    if numerical_df[column].nunique() > 2:\n","        Q1 = numerical_df[column].quantile(0.25)\n","        Q3 = numerical_df[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","        outliers_iqr = numerical_df[column][(numerical_df[column] < lower_bound) | (numerical_df[column] > upper_bound)]\n","        if not outliers_iqr.empty:\n","            print(f\"\\nPotential outliers in '{column}' (IQR method):\\n{outliers_iqr.head()}\") # head function only shows 5 examples of the outliners\n","        else:\n","            print(f\"\\nNo significant outliers found in '{column}' using IQR method.\")\n","    else:\n","        print(f\"\\nSkipping IQR outlier detection for binary column '{column}'.\")\n","\n","print(\"\\n7. Removing Outliers (IQR Method, Non-Binary Columns):\")\n","original_row_count = len(df)\n","rows_removed_per_column = {}\n","df_cleaned = df.copy()  # Create a copy of the DataFrame to avoid modifying the original during iteration\n","\n","# 'Ever_ChargeOff' is our target and 'Unpaid_Collection_Trades' has a limited range (0-3)\n","exclude_cols_outlier = ['Unpaid_Collection_Trades', 'Ever_ChargeOff', 'ChargeOff_Balance']\n","\n","# Iterate through columns in numerical_df that are NOT in the exclude list\n","# This is the corrected loop structure:\n","for column in numerical_df.columns:\n","    if column in exclude_cols_outlier:\n","        print(f\"\\nSkipping outlier removal for explicitly excluded column: '{column}'.\")\n","        continue # Skip to the next column in the loop\n","\n","    # Now apply your existing logic for outlier removal if not excluded and not binary\n","    if df_cleaned[column].nunique() > 2: # Use df_cleaned here as it's the one being modified\n","        Q1 = df_cleaned[column].quantile(0.25)\n","        Q3 = df_cleaned[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","\n","        outliers_mask = (df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound)\n","        outlier_indices = df_cleaned[outliers_mask].index\n","        rows_removed = len(outlier_indices)\n","\n","        if rows_removed > 0:\n","            df_cleaned = df_cleaned.drop(outlier_indices)\n","            rows_removed_per_column[column] = rows_removed\n","            print(f\"\\nRemoved {rows_removed} rows with IQR outliers in column '{column}'.\")\n","        else:\n","            print(f\"\\nNo IQR outliers found in column '{column}'.\")\n","    else:\n","        print(f\"\\nSkipping outlier removal for binary column '{column}'.\")\n","\n","\n","print(f\"\\n--- Summary of Rows Removed ---\")\n","total_rows_removed = original_row_count - len(df_cleaned)\n","print(f\"Original number of rows: {original_row_count}\")\n","print(f\"Number of rows after outlier removal: {len(df_cleaned)}\")\n","print(f\"Total number of rows removed: {total_rows_removed}\")\n","print(\"\\nRows removed per column (if any):\")\n","for column, count in rows_removed_per_column.items():\n","    print(f\"- '{column}': {count}\")\n","\n","print(\"\\nCleaned DataFrame (first 5 rows):\")\n","print(df_cleaned.head())\n","\n","# 7. Risk with missing data in loan data\n","print(\"\\n8. Risk with Missing Data in Loan Data:\")\n","print(\"\\nMissing data is a common problem in real-world datasets. In loan data, missing values can introduce bias and reduce the accuracy of analyses and models.\")\n","print(\"The risk associated with missing data depends on the extent and pattern of missingness. For example:\")\n","print(\"- Missing values in crucial features like income or credit score can significantly impact credit risk assessment.\")\n","print(\"- If missingness is systematic (related to other variables), it can introduce bias.\")\n","print(\"- High percentages of missing values in a column might render machine learning model prediction feature unreliable.\")\n","\n","# 8. Replacing missing credit data\n","print(\"\\n9. Replacing Missing Credit Data:\")\n","print(\"\\nSeveral techniques can be used to handle missing data:\")\n","print(\"- Imputation: Filling missing values with estimated values (e.g., mean, median, mode).\")\n","print(\"- More sophisticated imputation techniques (e.g., using machine learning models).\")\n","\n","print(\"\\nLet's check for missing values:\")\n","print(df.isnull().sum().sort_values(ascending=False))\n","\n","\n","from sklearn.model_selection import train_test_split\n","import xgboost as xgb\n","\n","\n","# Assuming your DataFrame after cleaning is named 'df_cleaned'\n","\n","print(\"\\n10. Split data into Train, Validation, and Test Data:\")\n","\n","# Split the data into training, validation, and test sets\n","train_df, temp_df = train_test_split(df_cleaned, test_size=0.3, random_state=42) # 30% go to validation-test, 70% remain for training\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)     #break-down 50/50 of 30% for validation and test\n","\n","print(f\"Training data shape: {train_df.shape}\")\n","print(f\"Validation data shape: {val_df.shape}\")\n","print(f\"Test data shape: {test_df.shape}\")\n","\n","print(\"\\n--- Explanation of Data Splitting ---\")\n","print(\"\"\"\n","We split our data into three distinct datasets: training, validation, and test. This is a fundamental practice in machine learning to build robust and reliable models.\n","\n","**1. Training Data:**\n","   - This is the largest portion of our data and the one that the machine learning model directly learns from.\n","   - The model will adjust its internal parameters based on the patterns and relationships it finds in the training data.\n","   - Think of it as the 'textbook' the model studies to understand the underlying concepts.\n","\n","**2. Validation Data:**\n","   - This dataset is used to tune the model's hyperparameters and to get an unbiased estimate of the model's performance *during* the training process.\n","   - Hyperparameters are settings of the model that are not learned from the data but are set prior to training (e.g., the learning rate of an algorithm, the depth of a decision tree).\n","   - By evaluating the model on the validation set after each training epoch or after trying different hyperparameter settings, we can see how well the model generalizes to unseen data and avoid overfitting (where the model learns the training data too well and performs poorly on new data).\n","   - The validation set acts as a 'practice exam' that helps us make adjustments to the model before the final evaluation.\n","\n","**3. Test Data:**\n","   - This is a completely separate dataset that the model *never* sees during the training or hyperparameter tuning phases.\n","   - It serves as the final, unbiased evaluation of the model's performance on completely new, unseen data.\n","   - The test set simulates how well the model would perform in a real-world scenario.\n","   - We only evaluate the model on the test set *once*, after we have finalized our model through training and validation.\n","   - Think of the test set as the 'final exam' that gives us a true measure of the model's capabilities.\n","\n","By using this three-way split, we can build a model that not only learns from the data but also generalizes well to new data and provides a reliable estimate of its real-world performance.\n","\"\"\")\n","\n","# Impute missing FICO in the training data with a special value (e.g., 0)\n","special_fico_value = 999\n","train_df['FICO_Imputed'] = train_df['FICO'].fillna(special_fico_value).astype(int)\n","\n","# Verify the imputation in the training data\n","print(\"\\nMissing FICO values in training data before imputation:\", train_df['FICO'].isnull().sum())\n","print(\"Missing FICO values in training data after imputation:\", train_df['FICO_Imputed'].isnull().sum())\n","print(\"First few rows of training data with imputed FICO:\")\n","print(train_df[['FICO', 'FICO_Imputed', 'No_Hit']].head())\n","\n","# For demonstration, let's also do the same for validation and test sets *independently*\n","# In a real project, you would apply the *same* special value to these as well.\n","val_df['FICO_Imputed'] = val_df['FICO'].fillna(special_fico_value).astype(int)\n","test_df['FICO_Imputed'] = test_df['FICO'].fillna(special_fico_value).astype(int)\n","\n","print(\"\\nMissing FICO values in validation data after imputation:\", val_df['FICO_Imputed'].isnull().sum())\n","print(\"Missing FICO values in test data after imputation:\", test_df['FICO_Imputed'].isnull().sum())\n","\n","# now check Ever_ChargeOff rate in train data\n","print(\"\\nValue Frequency of Ever_ChargeOff in Train Data:\")\n","for column in train_df.columns:\n","    if column == 'Ever_ChargeOff':\n","        print(f\"\\nValue counts for Ever_ChargeOff '{column}':\")\n","        print(train_df[column].value_counts())\n","print(\"\\nSome noticeable insight:  17%.1 ever charge-off, reasonable for NTC, young adult\")\n","# remember to check whether undersampling for training test is needed\n","# At 4%, we're in the moderate imbalance zone. No need to undersample — as 20% is isually considered as stastically important — but we still need to address imbalance through model-aware techniques.\n","\n","###### END of EDA #######\n","\n","###### --- MODELING: Predicting Ever_ChargeOff --- ######\n","\n","print(\"\\n=== Step 11: Building ML Model to Predict Ever_ChargeOff ===\")\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.preprocessing import StandardScaler\n","import xgboost as xgb # Import xgboost\n","\n","# Drop target and non-informative columns\n","# Also drop FICO_V and No_Hit to avoid redundancy (FICO_Imputed encodes both)\n","drop_cols = ['Ever_ChargeOff', 'FICO', 'No_Hit']\n","feature_cols = [col for col in train_df.columns if col not in drop_cols]\n","\n","\n","# 2. Prepare X and y\n","X_train = train_df[feature_cols]\n","y_train = train_df[target_binary]\n","\n","X_val = val_df[feature_cols]\n","y_val = val_df[target_binary]\n","\n","X_test = test_df[feature_cols]\n","y_test = test_df[target_binary]\n","\n","# 3. Fit an XGBoost Classifier\n","xgb_model = xgb.XGBClassifier(\n","    n_estimators=100,\n","    max_depth=6,\n","    learning_rate=0.1,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    use_label_encoder=False,\n","    eval_metric='logloss',\n","    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n","    random_state=42 # Add random_state for reproducibility\n",")\n","\n","xgb_model.fit(X_train, y_train)\n","\n","# Predictions on validation\n","val_probs_xgb = xgb_model.predict_proba(X_val)[:, 1]\n","val_preds_xgb = (val_probs_xgb >= 0.5).astype(int)\n","\n","# Evaluation on validation\n","val_auc_xgb = roc_auc_score(y_val, val_probs_xgb)\n","val_report_xgb = classification_report(y_val, val_preds_xgb, output_dict=True)\n","val_conf_xgb = confusion_matrix(y_val, val_preds_xgb)\n","\n","# ROC Curve\n","fpr_xgb, tpr_xgb, _ = roc_curve(y_val, val_probs_xgb)\n","\n","# Test set evaluation\n","test_probs_xgb = xgb_model.predict_proba(X_test)[:, 1]\n","test_preds_xgb = (test_probs_xgb >= 0.5).astype(int)\n","test_auc_xgb = roc_auc_score(y_test, test_probs_xgb)\n","test_report_xgb = classification_report(y_test, test_preds_xgb, output_dict=True)\n","test_conf_xgb = confusion_matrix(y_test, test_preds_xgb)\n","\n","# Visualize ROC curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost AUC = {val_auc_xgb:.2f}')\n","plt.plot([0, 1], [0, 1], linestyle='--')\n","plt.title('ROC Curve - XGBoost (Validation)')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","import ace_tools as tools; tools.display_dataframe_to_user(name=\"XGBoost Evaluation - Validation\", dataframe=pd.DataFrame(val_report_xgb).T)\n","# 3. Fit a Random Forest Classifier\n","rf_model = RandomForestClassifier(\n","    n_estimators=100,    #  Train 100 decision trees in the ensemble. More trees improve stability and generalization. 100 is a common, reliable starting point for performance without being too slow.\n","    max_depth=6,         #  Each tree can go only 6 levels deep. Prevents overfitting by limiting tree complexity. Shallow trees generalize better. Helps model stay interpretable and faster. Depth 6 is a good balance for our credit data (likely enough to model interactions without memorizing).\n","    class_weight='balanced',  # Automatically adjusts weights to compensate for class imbalance. Without balancing, the model might ignore the minority class. This tells the algorithm to treat both classes as equally important by assigning higher weight to the rare class.\n","    random_state=42  # fixed seed for reproducibility\n",")\n","rf_model.fit(X_train, y_train)\n","\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","import matplotlib.pyplot as plt\n","\n","# Train a shallow decision tree for easy visualization\n","tree = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=42)\n","tree.fit(X_train, y_train)\n","\n","# Plot the decision tree\n","plt.figure(figsize=(14, 8))\n","plot_tree(tree, feature_names=X_train.columns, class_names=[\"No Charge-off\", \"Charge-off\"], filled=True)\n","plt.title(\"Sample Decision Tree (Max Depth = 3)\")\n","plt.show()\n","#from the plot, at the root, we see gini at 0.5 meaning the highest uncertainty, getting down the tree, we see gini decrease\n","\n","# 4. Predict the target variable \"Ever_ChargeOff\" on validation data\n","val_probs = rf_model.predict_proba(X_val)[:, 1]\n","val_pred_thresh = (val_probs >= 0.5).astype(int)\n","# for each application, model will print out the probability of default as a Percentage number:\n","# Use threshold 0.3 for classification. Anyone with a (default 50%) 30% or higher chance of charge-off is now flagged as a predicted charge-off (class 1)\n","\n","# 5. Evaluate model performance using thresholded predictions\n","print(\"\\n--- Validation Performance ---\")\n","print(confusion_matrix(y_val, val_pred_thresh))\n","print(classification_report(y_val, val_pred_thresh, digits=3))\n","\n","val_auc = roc_auc_score(y_val, val_probs)\n","print(f\"Validation AUC: {val_auc:.3f}\")\n","\n","# 6. ROC Curve (unchanged, uses probabilities)\n","fpr, tpr, thresholds = roc_curve(y_val, val_probs)\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f\"AUC = {val_auc:.2f}\")\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve on Validation Set\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","# 7. Evaluate on Test Set (can remain unchanged if no thresholding applied here)\n","test_preds = rf_model.predict(X_test)\n","test_probs = rf_model.predict_proba(X_test)[:, 1]\n","\n","print(\"\\n--- Test Set Performance ---\")\n","print(confusion_matrix(y_test, test_preds))\n","print(classification_report(y_test, test_preds, digits=3))\n","print(f\"Test AUC: {roc_auc_score(y_test, test_probs):.3f}\")\n","\n","print(\"\\nAs our intent of applying business acumen of prioritize detecting charge-off, our model with decent 60% Recall with test data, but in ex-change for a low precision and F-1, meaning we are auto-decline more people than trying to play risk with accepting them\")\n","print(\"\\nThis model is suitable for business that have high tolerance in giving our extra manual review and. Our model working as designed\")\n","\n","# 8. Feature importance (unchanged)\n","importances = rf_model.feature_importances_\n","feat_imp_df = pd.DataFrame({'Feature': feature_cols, 'Importance': importances})\n","feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(data=feat_imp_df.head(15), x='Importance', y='Feature')\n","plt.title(\"Top 15 Feature Importances (Random Forest)\")\n","plt.tight_layout()\n","plt.show()\n","print(\"\\nValue counts of categorical features:\")\n","for column in df_cleaned.select_dtypes(include='object').columns: # Check cleaned df\n","    print(f\"\\nValue counts for column '{column}':\")\n","    print(df_cleaned[column].value_counts())\n","\n","# XGBoost: nureau network, cautious: overfiting, based off current dataset: 26 variables\n","# compare Random Forest and XGBoost : thi thang nao co precision cao hon, AUC cao hon.\n","\n","\n"]},{"source":["from IPython import get_ipython\n","from IPython.display import display\n","# %%\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from google.colab import drive\n","# Use force_remount=True to ensure the drive is mounted even if it was already mounted\n","drive.mount('/content/drive', force_remount=True)\n","# Load the credit profile data\n","df = pd.read_csv(/content/test.csv)\n","# df = pd.read_csv('/content/drive/MyDrive/Huy_ACE/Huy_Projects/test.csv')\n","print(\"Data loaded successfully!\")\n","\n","# Drop the ID column if it exists\n","if 'ID' in df.columns:\n","    df = df.drop('ID', axis=1)\n","    print(\"ID column dropped.\")\n","else:\n","    print(\"ID column not found.\")\n","# Drop the ID column if it exists\n","if 'Unpaid_CO_84M ' in df.columns:\n","    df = df.drop('Unpaid_CO_84M ', axis=1)\n","    print(\"Unpaid_CO_84M  column dropped.\")\n","else:\n","    print(\"Unpaid_CO_84M  column not found.\")\n","# --- EDA Framework ---\n","\n","# Set options to display all columns and a large number of rows\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","\n","print(\"\\n--- EDA: Exploring Credit Risk Data ---\")\n","\n","# 1. Discuss the concept of consumer credit risk and define how it is calculated.\n","print(\"\\n1. Credit Risk Concept and Calculation:\")\n","print(\"Credit risk is the possibility of a borrower failing to repay a loan or meet their debt obligations. It's a crucial concern for lenders as it directly impacts their profitability.\")\n","print(\"Credit risk assessment often involves evaluating a borrower's creditworthiness based on factors like credit history, repayment capacity, income, loan terms, and collateral.\")\n","print(\"Quantifying credit risk can involve calculating metrics such as:\")\n","print(\"- Probability of Default (PD%): The likelihood that a borrower will default on their debt.\")\n","print(\"- Net Credit Loss ($NCL%): The total amount the lender is at risk for at the time of default.\")\n","\n","# 2. Explore the credit data\n","print(\"\\n2. Exploring the Credit Data:\")\n","print(\"\\nFirst, let's get a general overview of the data:\")\n","print(df.head())\n","print(\"\\nDataframe information:\")\n","df.info()\n","print(\"\\nSummary statistics of numerical features:\")\n","print(df.describe())\n","print(\"\\nValue counts of categorical features:\")\n","for column in df.select_dtypes(include='object').columns:\n","    print(f\"\\nValue counts for column '{column}':\")\n","    print(df[column].value_counts())\n","\n","# 3. Crosstab and pivot tables with two target variables\n","print(\"\\n3. Correlation Heatmaps with Target Variables Ever_ChargeOff and ChargeOff_Balance:\")\n","print(\"\\nTo simplify the analysis of correlations, we will generate two heatmaps showing the correlation of all numerical variables with each of the target variables.\")\n","\n","target_binary = 'Ever_ChargeOff'\n","target_continuous = 'ChargeOff_Balance'\n","\n","numerical_df = df.select_dtypes(include=np.number).copy()\n","\n","# Correlation with Ever_ChargeOff\n","correlation_with_ever_chargeoff = numerical_df.corr()[target_binary].sort_values(ascending=False)\n","\n","# Display the correlation table for Ever_ChargeOff\n","print(\"\\nCorrelation Table with Ever_ChargeOff:\")\n","print(correlation_with_ever_chargeoff.to_frame())\n","\n","# Correlation heatmap for Ever_ChargeOff\n","plt.figure(figsize=(8, 10))\n","sns.heatmap(correlation_with_ever_chargeoff.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","plt.title(f'Correlation with Ever_ChargeOff')\n","plt.show()\n","\n","# Correlation with ChargeOff_Balance\n","correlation_with_chargeoff_balance = numerical_df.corr()[target_continuous].sort_values(ascending=False)\n","\n","# Display the correlation table for ChargeOff_Balance\n","print(\"\\nCorrelation Table with ChargeOff_Balance:\")\n","print(correlation_with_chargeoff_balance.to_frame())\n","\n","# Correlation heatmap for ChargeOff_Balance\n","plt.figure(figsize=(8, 10))\n","sns.heatmap(correlation_with_chargeoff_balance.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","plt.title(f'Correlation with ChargeOff_Balance')\n","plt.show()\n","\n","# Drop the ChargeOff_Balance column if it exists due to abnormal high miss-match of Ever_ChargeOff and ChargeOff_Balance\n","if 'ChargeOff_Balance' in df.columns:\n","    df = df.drop('ChargeOff_Balance', axis=1)\n","    print(\"ChargeOff_Balance column dropped.\")\n","else:\n","    print(\"ChargeOff_Balance column not found.\")\n","\n","# 4. Outliers in credit data\n","print(\"\\n4. Outliers in Credit Data:\")\n","print(\"\\nOutliers are data points that significantly deviate from the rest of the data. In credit data, outliers can represent unusual applicant profiles or errors in data entry. They can skew statistical analyses and affect machine learning model performance.\")\n","\n","# 5. Finding outliers with cross tables\n","print(\"\\n5. Finding Outliers with Cross Tables:\")\n","print(\"\\nWhile cross tables primarily show relationships between categorical variables, unusual distributions within categories might hint at outliers in related numerical variables.\")\n","print(\"For example, if a specific category has a very high or low average loan amount compared to others, it could indicate potential outliers.\")\n","\n","print(\"\\nValue Frequency of Binary Columns:\")\n","for column in df.columns:\n","    if df[column].nunique() == 2:\n","        print(f\"\\nValue counts for binary column '{column}':\")\n","        print(df[column].value_counts())\n","print(\"\\nSome noticeable insight: 1.4% went bankruptcy before, 20% ever collection, meanwhile, 14%.1 ever charge-off, reasonable for NTC, young adult\")\n","\n","print(\"\\nExamining Distributions with Histograms and KDE Plots:\")\n","for column in numerical_df.columns:\n","  if numerical_df[column].nunique() > 2:\n","     plt.figure(figsize=(8, 6))\n","     sns.histplot(numerical_df[column], kde=True)\n","     plt.title(f'Distribution of {column}')\n","     plt.show()\n","  else:\n","      print(f\"\\nSkipping box plot for '{column}' as it is binary.\")\n","\n","\n","print(\"\\nFor column with null values like FICO, the histogram only distibution for populated values\")\n","\n","print(\"\\nHistogram is great to give overview for how values are distributed, but need 1.5Q Method to effetively identify outliner\")\n","print(\"\\n6.Identifying Outliers using IQR Method (Non-Binary Columns):\")\n","for column in numerical_df.columns:\n","    if numerical_df[column].nunique() > 2:\n","        Q1 = numerical_df[column].quantile(0.25)\n","        Q3 = numerical_df[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","        outliers_iqr = numerical_df[column][(numerical_df[column] < lower_bound) | (numerical_df[column] > upper_bound)]\n","        if not outliers_iqr.empty:\n","            print(f\"\\nPotential outliers in '{column}' (IQR method):\\n{outliers_iqr.head()}\") # head function only shows 5 examples of the outliners\n","        else:\n","            print(f\"\\nNo significant outliers found in '{column}' using IQR method.\")\n","    else:\n","        print(f\"\\nSkipping IQR outlier detection for binary column '{column}'.\")\n","\n","print(\"\\n7. Removing Outliers (IQR Method, Non-Binary Columns):\")\n","original_row_count = len(df)\n","rows_removed_per_column = {}\n","df_cleaned = df.copy()  # Create a copy of the DataFrame to avoid modifying the original during iteration\n","\n","# 'Ever_ChargeOff' is our target and 'Unpaid_Collection_Trades' has a limited range (0-3)\n","exclude_cols_outlier = ['Unpaid_Collection_Trades', 'Ever_ChargeOff', 'ChargeOff_Balance']\n","\n","# Iterate through columns in numerical_df that are NOT in the exclude list\n","# This is the corrected loop structure:\n","for column in numerical_df.columns:\n","    if column in exclude_cols_outlier:\n","        print(f\"\\nSkipping outlier removal for explicitly excluded column: '{column}'.\")\n","        continue # Skip to the next column in the loop\n","\n","    # Now apply your existing logic for outlier removal if not excluded and not binary\n","    if df_cleaned[column].nunique() > 2: # Use df_cleaned here as it's the one being modified\n","        Q1 = df_cleaned[column].quantile(0.25)\n","        Q3 = df_cleaned[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","\n","        outliers_mask = (df_cleaned[column] < lower_bound) | (df_cleaned[column] > upper_bound)\n","        outlier_indices = df_cleaned[outliers_mask].index\n","        rows_removed = len(outlier_indices)\n","\n","        if rows_removed > 0:\n","            df_cleaned = df_cleaned.drop(outlier_indices)\n","            rows_removed_per_column[column] = rows_removed\n","            print(f\"\\nRemoved {rows_removed} rows with IQR outliers in column '{column}'.\")\n","        else:\n","            print(f\"\\nNo IQR outliers found in column '{column}'.\")\n","    else:\n","        print(f\"\\nSkipping outlier removal for binary column '{column}'.\")\n","\n","\n","print(f\"\\n--- Summary of Rows Removed ---\")\n","total_rows_removed = original_row_count - len(df_cleaned)\n","print(f\"Original number of rows: {original_row_count}\")\n","print(f\"Number of rows after outlier removal: {len(df_cleaned)}\")\n","print(f\"Total number of rows removed: {total_rows_removed}\")\n","print(\"\\nRows removed per column (if any):\")\n","for column, count in rows_removed_per_column.items():\n","    print(f\"- '{column}': {count}\")\n","\n","print(\"\\nCleaned DataFrame (first 5 rows):\")\n","print(df_cleaned.head())\n","\n","# 7. Risk with missing data in loan data\n","print(\"\\n8. Risk with Missing Data in Loan Data:\")\n","print(\"\\nMissing data is a common problem in real-world datasets. In loan data, missing values can introduce bias and reduce the accuracy of analyses and models.\")\n","print(\"The risk associated with missing data depends on the extent and pattern of missingness. For example:\")\n","print(\"- Missing values in crucial features like income or credit score can significantly impact credit risk assessment.\")\n","print(\"- If missingness is systematic (related to other variables), it can introduce bias.\")\n","print(\"- High percentages of missing values in a column might render machine learning model prediction feature unreliable.\")\n","\n","# 8. Replacing missing credit data\n","print(\"\\n9. Replacing Missing Credit Data:\")\n","print(\"\\nSeveral techniques can be used to handle missing data:\")\n","print(\"- Imputation: Filling missing values with estimated values (e.g., mean, median, mode).\")\n","print(\"- More sophisticated imputation techniques (e.g., using machine learning models).\")\n","\n","print(\"\\nLet's check for missing values:\")\n","print(df.isnull().sum().sort_values(ascending=False))\n","\n","\n","from sklearn.model_selection import train_test_split\n","import xgboost as xgb\n","\n","\n","# Assuming your DataFrame after cleaning is named 'df_cleaned'\n","\n","print(\"\\n10. Split data into Train, Validation, and Test Data:\")\n","\n","# Split the data into training, validation, and test sets\n","train_df, temp_df = train_test_split(df_cleaned, test_size=0.3, random_state=42) # 30% go to validation-test, 70% remain for training\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)     #break-down 50/50 of 30% for validation and test\n","\n","print(f\"Training data shape: {train_df.shape}\")\n","print(f\"Validation data shape: {val_df.shape}\")\n","print(f\"Test data shape: {test_df.shape}\")\n","\n","print(\"\\n--- Explanation of Data Splitting ---\")\n","print(\"\"\"\n","We split our data into three distinct datasets: training, validation, and test. This is a fundamental practice in machine learning to build robust and reliable models.\n","\n","**1. Training Data:**\n","   - This is the largest portion of our data and the one that the machine learning model directly learns from.\n","   - The model will adjust its internal parameters based on the patterns and relationships it finds in the training data.\n","   - Think of it as the 'textbook' the model studies to understand the underlying concepts.\n","\n","**2. Validation Data:**\n","   - This dataset is used to tune the model's hyperparameters and to get an unbiased estimate of the model's performance *during* the training process.\n","   - Hyperparameters are settings of the model that are not learned from the data but are set prior to training (e.g., the learning rate of an algorithm, the depth of a decision tree).\n","   - By evaluating the model on the validation set after each training epoch or after trying different hyperparameter settings, we can see how well the model generalizes to unseen data and avoid overfitting (where the model learns the training data too well and performs poorly on new data).\n","   - The validation set acts as a 'practice exam' that helps us make adjustments to the model before the final evaluation.\n","\n","**3. Test Data:**\n","   - This is a completely separate dataset that the model *never* sees during the training or hyperparameter tuning phases.\n","   - It serves as the final, unbiased evaluation of the model's performance on completely new, unseen data.\n","   - The test set simulates how well the model would perform in a real-world scenario.\n","   - We only evaluate the model on the test set *once*, after we have finalized our model through training and validation.\n","   - Think of the test set as the 'final exam' that gives us a true measure of the model's capabilities.\n","\n","By using this three-way split, we can build a model that not only learns from the data but also generalizes well to new data and provides a reliable estimate of its real-world performance.\n","\"\"\")\n","\n","# Impute missing FICO in the training data with a special value (e.g., 0)\n","special_fico_value = 999\n","train_df['FICO_Imputed'] = train_df['FICO'].fillna(special_fico_value).astype(int)\n","\n","# Verify the imputation in the training data\n","print(\"\\nMissing FICO values in training data before imputation:\", train_df['FICO'].isnull().sum())\n","print(\"Missing FICO values in training data after imputation:\", train_df['FICO_Imputed'].isnull().sum())\n","print(\"First few rows of training data with imputed FICO:\")\n","print(train_df[['FICO', 'FICO_Imputed', 'No_Hit']].head())\n","\n","# For demonstration, let's also do the same for validation and test sets *independently*\n","# In a real project, you would apply the *same* special value to these as well.\n","val_df['FICO_Imputed'] = val_df['FICO'].fillna(special_fico_value).astype(int)\n","test_df['FICO_Imputed'] = test_df['FICO'].fillna(special_fico_value).astype(int)\n","\n","print(\"\\nMissing FICO values in validation data after imputation:\", val_df['FICO_Imputed'].isnull().sum())\n","print(\"Missing FICO values in test data after imputation:\", test_df['FICO_Imputed'].isnull().sum())\n","\n","# now check Ever_ChargeOff rate in train data\n","print(\"\\nValue Frequency of Ever_ChargeOff in Train Data:\")\n","for column in train_df.columns:\n","    if column == 'Ever_ChargeOff':\n","        print(f\"\\nValue counts for Ever_ChargeOff '{column}':\")\n","        print(train_df[column].value_counts())\n","print(\"\\nSome noticeable insight:  17%.1 ever charge-off, reasonable for NTC, young adult\")\n","# remember to check whether undersampling for training test is needed\n","# At 4%, we're in the moderate imbalance zone. No need to undersample — as 20% is isually considered as stastically important — but we still need to address imbalance through model-aware techniques.\n","\n","###### END of EDA #######\n","\n","###### --- MODELING: Predicting Ever_ChargeOff --- ######\n","\n","print(\"\\n=== Step 11: Building ML Model to Predict Ever_ChargeOff ===\")\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.preprocessing import StandardScaler\n","import xgboost as xgb # Import xgboost\n","\n","# Drop target and non-informative columns\n","# Also drop FICO_V and No_Hit to avoid redundancy (FICO_Imputed encodes both)\n","drop_cols = ['Ever_ChargeOff', 'FICO', 'No_Hit']\n","feature_cols = [col for col in train_df.columns if col not in drop_cols]\n","\n","\n","# 2. Prepare X and y\n","X_train = train_df[feature_cols]\n","y_train = train_df[target_binary]\n","\n","X_val = val_df[feature_cols]\n","y_val = val_df[target_binary]\n","\n","X_test = test_df[feature_cols]\n","y_test = test_df[target_binary]\n","\n","# 3. Fit an XGBoost Classifier\n","xgb_model = xgb.XGBClassifier(\n","    n_estimators=100,\n","    max_depth=6,\n","    learning_rate=0.1,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    use_label_encoder=False,\n","    eval_metric='logloss',\n","    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n","    random_state=42 # Add random_state for reproducibility\n",")\n","\n","xgb_model.fit(X_train, y_train)\n","\n","# Predictions on validation\n","val_probs_xgb = xgb_model.predict_proba(X_val)[:, 1]\n","val_preds_xgb = (val_probs_xgb >= 0.5).astype(int)\n","\n","# Evaluation on validation\n","val_auc_xgb = roc_auc_score(y_val, val_probs_xgb)\n","val_report_xgb = classification_report(y_val, val_preds_xgb, output_dict=True)\n","val_conf_xgb = confusion_matrix(y_val, val_preds_xgb)\n","\n","# ROC Curve\n","fpr_xgb, tpr_xgb, _ = roc_curve(y_val, val_probs_xgb)\n","\n","# Test set evaluation\n","test_probs_xgb = xgb_model.predict_proba(X_test)[:, 1]\n","test_preds_xgb = (test_probs_xgb >= 0.5).astype(int)\n","test_auc_xgb = roc_auc_score(y_test, test_probs_xgb)\n","test_report_xgb = classification_report(y_test, test_preds_xgb, output_dict=True)\n","test_conf_xgb = confusion_matrix(y_test, test_preds_xgb)\n","\n","# Visualize ROC curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost AUC = {val_auc_xgb:.2f}')\n","plt.plot([0, 1], [0, 1], linestyle='--')\n","plt.title('ROC Curve - XGBoost (Validation)')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","import ace_tools as tools; tools.display_dataframe_to_user(name=\"XGBoost Evaluation - Validation\", dataframe=pd.DataFrame(val_report_xgb).T)\n","# 3. Fit a Random Forest Classifier\n","rf_model = RandomForestClassifier(\n","    n_estimators=100,    #  Train 100 decision trees in the ensemble. More trees improve stability and generalization. 100 is a common, reliable starting point for performance without being too slow.\n","    max_depth=6,         #  Each tree can go only 6 levels deep. Prevents overfitting by limiting tree complexity. Shallow trees generalize better. Helps model stay interpretable and faster. Depth 6 is a good balance for our credit data (likely enough to model interactions without memorizing).\n","    class_weight='balanced',  # Automatically adjusts weights to compensate for class imbalance. Without balancing, the model might ignore the minority class. This tells the algorithm to treat both classes as equally important by assigning higher weight to the rare class.\n","    random_state=42  # fixed seed for reproducibility\n",")\n","rf_model.fit(X_train, y_train)\n","\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","import matplotlib.pyplot as plt\n","\n","# Train a shallow decision tree for easy visualization\n","tree = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=42)\n","tree.fit(X_train, y_train)\n","\n","# Plot the decision tree\n","plt.figure(figsize=(14, 8))\n","plot_tree(tree, feature_names=X_train.columns, class_names=[\"No Charge-off\", \"Charge-off\"], filled=True)\n","plt.title(\"Sample Decision Tree (Max Depth = 3)\")\n","plt.show()\n","#from the plot, at the root, we see gini at 0.5 meaning the highest uncertainty, getting down the tree, we see gini decrease\n","\n","# 4. Predict the target variable \"Ever_ChargeOff\" on validation data\n","val_probs = rf_model.predict_proba(X_val)[:, 1]\n","val_pred_thresh = (val_probs >= 0.5).astype(int)\n","# for each application, model will print out the probability of default as a Percentage number:\n","# Use threshold 0.3 for classification. Anyone with a (default 50%) 30% or higher chance of charge-off is now flagged as a predicted charge-off (class 1)\n","\n","# 5. Evaluate model performance using thresholded predictions\n","print(\"\\n--- Validation Performance ---\")\n","print(confusion_matrix(y_val, val_pred_thresh))\n","print(classification_report(y_val, val_pred_thresh, digits=3))\n","\n","val_auc = roc_auc_score(y_val, val_probs)\n","print(f\"Validation AUC: {val_auc:.3f}\")\n","\n","# 6. ROC Curve (unchanged, uses probabilities)\n","fpr, tpr, thresholds = roc_curve(y_val, val_probs)\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f\"AUC = {val_auc:.2f}\")\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve on Validation Set\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","# 7. Evaluate on Test Set (can remain unchanged if no thresholding applied here)\n","test_preds = rf_model.predict(X_test)\n","test_probs = rf_model.predict_proba(X_test)[:, 1]\n","\n","print(\"\\n--- Test Set Performance ---\")\n","print(confusion_matrix(y_test, test_preds))\n","print(classification_report(y_test, test_preds, digits=3))\n","print(f\"Test AUC: {roc_auc_score(y_test, test_probs):.3f}\")\n","\n","print(\"\\nAs our intent of applying business acumen of prioritize detecting charge-off, our model with decent 60% Recall with test data, but in ex-change for a low precision and F-1, meaning we are auto-decline more people than trying to play risk with accepting them\")\n","print(\"\\nThis model is suitable for business that have high tolerance in giving our extra manual review and. Our model working as designed\")\n","\n","# 8. Feature importance (unchanged)\n","importances = rf_model.feature_importances_\n","feat_imp_df = pd.DataFrame({'Feature': feature_cols, 'Importance': importances})\n","feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(data=feat_imp_df.head(15), x='Importance', y='Feature')\n","plt.title(\"Top 15 Feature Importances (Random Forest)\")\n","plt.tight_layout()\n","plt.show()\n","print(\"\\nValue counts of categorical features:\")\n","for column in df_cleaned.select_dtypes(include='object').columns: # Check cleaned df\n","    print(f\"\\nValue counts for column '{column}':\")\n","    print(df_cleaned[column].value_counts())\n","\n","# XGBoost: nureau network, cautious: overfiting, based off current dataset: 26 variables\n","# compare Random Forest and XGBoost : thi thang nao co precision cao hon, AUC cao hon."],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"0uSEN-kFaYgy","executionInfo":{"status":"error","timestamp":1748478295792,"user_tz":240,"elapsed":71,"user":{"displayName":"cao huy","userId":"01401569740397649543"}},"outputId":"59c3552a-70bd-4a00-a1b4-3aedeaa48c20"},"execution_count":9,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-9-e299a05486dd>, line 13)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-e299a05486dd>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    df = pd.read_csv(/content/test.csv)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"GWWEytoK51Ja"},"execution_count":null,"outputs":[]}]}